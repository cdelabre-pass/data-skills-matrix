# Stack Data - Orchestration skills
category:
  id: orchestration
  name: "Stack Data - Orchestration"

skills:
  - id: airflow
    name: "Apache Airflow"
    description: "Orchestration de workflows avec Airflow: DAGs, operators standards et custom, hooks, sensors, plugins, performances"
    core_roles: [data_engineer, analytics_eng, ml_engineer]
    levels:
      data_analyst: [NC, NC, 1, 2]
      data_engineer: [1, 2, 3, 4]
      data_scientist: [NC, NC, 1, 2]
      analytics_eng: [1, 2, 3, 4]
      ml_engineer: [1, 2, 3, 4]
      backend: [1, 1, 2, 3]
    level_descriptions:
      0: "Aucune connaissance Airflow"
      1: "Je navigue dans l'UI Airflow, je lis les DAGs existants et je comprends les operators standards"
      2: "J'écris des DAGs avec dépendances, scheduling et hooks pour les connexions externes"
      3: "Je conçois des workflows complexes, je développe des operators custom documentés et testés, et je crée des plugins"
      4: "J'optimise les performances d'Airflow à l'échelle, je configure les executors et je contribue à des providers"
      5: "Je définis la stratégie Airflow de l'organisation, je forme les équipes aux patterns avancés et j'établis les standards de qualité des DAGs"
      6: "Contributeur Apache Airflow, speaker Airflow Summit"
    behavioral_indicators:
      1:
        - "Je navigue dans l'UI Airflow et comprends les états des tasks"
        - "Je sais lire un DAG existant et comprendre ses dépendances"
        - "Je peux relancer manuellement un DAG ou une task"
        - "Je connais les operators standards (PythonOperator, BashOperator)"
      2:
        - "J'écris des DAGs avec plusieurs tasks et dépendances"
        - "Je configure le scheduling avec des cron expressions et les templates Jinja"
        - "J'utilise les hooks pour les connexions externes et les sensors"
        - "Je comprends l'héritage de BaseOperator et je modifie des operators existants"
      3:
        - "Je crée des DAGs dynamiques avec des factories"
        - "J'ai développé des operators custom documentés et testés"
        - "Je crée des plugins Airflow réutilisables et les publie dans un registry interne"
        - "J'implémente les patterns de retry et alerting"
      4:
        - "J'optimise les performances d'Airflow à l'échelle"
        - "Je configure les executors (Celery, Kubernetes) et je définis les conventions de l'équipe"
        - "Je produis les templates de DAGs et la documentation de référence pour l'équipe"
        - "J'ai contribué à un provider Airflow et conçu l'architecture des plugins"
      5:
        - "Je définis la stratégie Airflow de l'organisation"
        - "Je conçois les architectures de workflows de référence pour les cas complexes"
        - "Je forme les équipes aux patterns Airflow avancés et au développement d'operators"
        - "J'établis les standards de qualité, de monitoring et de réutilisabilité des DAGs et plugins"
      6:
        - "Je contribue au code source d'Apache Airflow ou aux providers officiels"
        - "Je suis speaker à Airflow Summit ou conférences data engineering"
        - "Je suis reconnu comme expert Airflow dans la communauté"
        - "Je publie des articles ou maintiens des providers Airflow open-source"
    resources:
      - url: "https://airflow.apache.org/docs/"
        title: "Apache Airflow Documentation"
        type: "documentation"
      - url: "https://www.astronomer.io/docs/learn"
        title: "Astronomer Learn"
        type: "tutorial"
      - url: "https://academy.astronomer.io/"
        title: "Astronomer Academy"
        type: "course"
      - url: "https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html"
        title: "Custom Operators Guide"
        type: "documentation"
    improvement_tips:
      "1→2": "Écrivez votre premier DAG avec plusieurs tasks et dépendances. Utilisez les hooks pour une connexion externe."
      "2→3": "Créez un operator custom documenté et testé. Développez un plugin réutilisable pour votre équipe."
      "3→4": "Optimisez les performances d'Airflow à l'échelle. Contribuez à un provider Apache Airflow."
