# Stack Data - Programmation skills
category:
  id: programming
  name: "Stack Data - Programmation"

skills:
  - id: python_data
    name: "Python (Data)"
    description: "Programmation Python pour le data (pandas, numpy, pyarrow, uv)"
    core_roles: [data_analyst, data_engineer, data_scientist, analytics_eng, ml_engineer, backend]
    levels:
      data_analyst: [1, 2, 3, 4]
      data_engineer: [2, 3, 4, 4]
      data_scientist: [2, 3, 4, 4]
      analytics_eng: [1, 2, 3, 4]
      ml_engineer: [2, 3, 4, 4]
      backend: [2, 3, 4, 4]
    level_descriptions:
      0: "Aucune connaissance Python"
      1: "J'écris des scripts basiques et j'utilise pandas simplement"
      2: "J'écris du code propre et j'utilise pandas/numpy efficacement"
      3: "Je construis des pipelines de données et je comprends les patterns de transformation complexes"
      4: "Je maîtrise Python en profondeur, j'écris du code maintenable et scalable et je comprends les mécanismes internes"
      5: "Je forme l'organisation aux bonnes pratiques Python et je définis les standards de code"
      6: "Contributeur open-source Python data ecosystem"
    behavioral_indicators:
      1:
        - "Je peux lire un CSV avec pandas et faire des filtres basiques"
        - "J'utilise des boucles for et des conditions if/else"
        - "Je sais créer des fonctions simples"
        - "Je cherche souvent la syntaxe sur Stack Overflow"
      2:
        - "J'utilise groupby, merge, pivot_table couramment"
        - "Je structure mon code en fonctions réutilisables"
        - "Je gère les types de données (datetime, categorical, etc.)"
        - "J'écris des list/dict comprehensions"
        - "Je sais utiliser numpy pour les opérations vectorisées"
      3:
        - "Je construis des pipelines de transformation modulaires"
        - "J'utilise pyarrow/polars pour les gros volumes"
        - "Je crée des packages Python avec pyproject.toml et uv"
        - "J'écris des tests unitaires pour mon code"
        - "J'utilise les type hints et la validation Pydantic"
      4:
        - "Je fais des code reviews Python pour l'équipe"
        - "J'ai défini les standards Python du projet"
        - "Je maîtrise les patterns avancés (decorators, context managers)"
        - "J'optimise les performances (profiling, memory)"
      5:
        - "Je définis les standards Python de l'organisation"
        - "J'arbitre les décisions d'architecture Python pour l'organisation"
        - "Je forme les équipes sur les patterns Python avancés"
        - "J'établis les conventions de code et les bonnes pratiques"
      6:
        - "Je contribue activement à des projets open-source (pandas, numpy, polars)"
        - "Je suis speaker à PyCon ou autres conférences Python"
        - "Je suis reconnu dans la communauté Python data"
        - "Je publie des articles ou packages Python populaires"
    resources:
      - url: "https://docs.python.org/3/tutorial/"
        title: "Python Official Tutorial"
        type: "tutorial"
      - url: "https://pandas.pydata.org/docs/getting_started/intro_tutorials/"
        title: "Pandas Getting Started"
        type: "tutorial"
      - url: "https://www.coursera.org/specializations/python"
        title: "Python for Everybody (Coursera)"
        type: "course"
      - url: "https://realpython.com/"
        title: "Real Python Tutorials"
        type: "tutorial"
    improvement_tips:
      "1→2": "Apprenez les opérations pandas avancées (groupby, merge, pivot). Écrivez du code avec des fonctions réutilisables."
      "2→3": "Maîtrisez les patterns de data pipelines. Apprenez pyarrow pour les gros volumes."
      "3→4": "Contribuez aux standards de code de l'équipe. Écrivez des packages Python internes documentés."

  - id: python_backend
    name: "Python Backend & APIs"
    description: "Consommer et construire des APIs REST avec Python (FastAPI, Flask): pagination, auth, intégration, patterns de résilience, architecture backend"
    core_roles: [backend, ml_engineer, data_engineer]
    levels:
      data_analyst: [1, 1, 2, 3]
      data_engineer: [1, 2, 3, 4]
      data_scientist: [1, 1, 2, 3]
      analytics_eng: [1, 1, 2, 3]
      ml_engineer: [1, 2, 3, 4]
      backend: [2, 3, 4, 4]
    level_descriptions:
      0: "Aucune connaissance API/backend Python"
      1: "Je fais des appels API basiques (requests, curl), je comprends REST et le format JSON"
      2: "Je construis des APIs simples avec FastAPI et je gère l'authentification, la pagination et les erreurs"
      3: "Je conçois des APIs scalables avec auth, caching et retry, et je crée des clients API robustes avec patterns de résilience"
      4: "Je maîtrise l'architecture backend et API en profondeur, je gère les scénarios complexes (webhooks, streaming)"
      5: "Je définis la stratégie API et backend de l'organisation et je forme les équipes aux bonnes pratiques"
      6: "Contributeur FastAPI/frameworks, expert API design reconnu"
    behavioral_indicators:
      1:
        - "Je fais des appels GET/POST avec requests ou curl"
        - "Je comprends les codes HTTP et le format JSON"
        - "Je peux lire la documentation d'une API et l'utiliser"
      2:
        - "J'ai créé une API CRUD avec FastAPI et Pydantic"
        - "Je gère l'authentification (API keys, tokens, JWT)"
        - "J'implémente la pagination et la gestion des erreurs HTTP"
        - "Je documente mes APIs avec OpenAPI/Swagger"
      3:
        - "J'implémente l'authentification OAuth et le caching (Redis)"
        - "Je crée des clients API robustes avec retry et rate limiting"
        - "Je structure mes projets avec une architecture clean"
        - "J'ai intégré plusieurs APIs externes avec patterns de résilience"
      4:
        - "Je conçois l'architecture API et backend de l'équipe"
        - "J'implémente le monitoring des APIs et les métriques de performance"
        - "Je gère les scénarios complexes (webhooks, streaming, concurrence)"
        - "Je produis les standards d'intégration et les patterns réutilisables pour l'équipe"
      5:
        - "Je définis la stratégie API et backend de l'organisation"
        - "J'arbitre les décisions d'architecture backend et d'intégration"
        - "Je forme les équipes sur les patterns API avancés et la résilience"
        - "J'établis les standards de sécurité, performance et robustesse"
      6:
        - "Je contribue activement à FastAPI ou autres frameworks Python"
        - "Je suis speaker à PyCon ou conférences API"
        - "Je contribue aux standards et spécifications API (OpenAPI, AsyncAPI)"
        - "Je publie des articles ou maintiens des projets open-source backend"
    resources:
      - url: "https://fastapi.tiangolo.com/tutorial/"
        title: "FastAPI Tutorial"
        type: "tutorial"
      - url: "https://testdriven.io/courses/tdd-fastapi/"
        title: "Test-Driven Development with FastAPI"
        type: "course"
      - url: "https://12factor.net/"
        title: "The Twelve-Factor App"
        type: "documentation"
      - url: "https://swagger.io/specification/"
        title: "OpenAPI Specification"
        type: "documentation"
    improvement_tips:
      "1→2": "Créez une API CRUD avec FastAPI. Intégrez une API externe avec authentification et pagination."
      "2→3": "Implémentez l'authentification OAuth et le caching. Créez un client API robuste avec retry et rate limiting."
      "3→4": "Concevez une architecture API complète avec monitoring. Définissez les standards d'intégration de votre équipe."

  - id: etl_connectors
    name: "Connecteurs ETL"
    description: "Développer des connecteurs vers des sources externes (APIs SaaS, fichiers)"
    core_roles: [data_engineer]
    levels:
      data_analyst: [NC, NC, 1, 2]
      data_engineer: [1, 2, 3, 4]
      data_scientist: [NC, NC, 1, 2]
      analytics_eng: [1, 1, 2, 3]
      ml_engineer: [1, 2, 3, 4]
      backend: [1, 2, 3, 4]
    level_descriptions:
      0: "Aucune connaissance ETL"
      1: "Je comprends les patterns d'extraction de données"
      2: "Je développe des connecteurs basiques vers des APIs"
      3: "Je conçois des connecteurs robustes avec retry et error handling et je comprends les patterns de résilience"
      4: "Je maîtrise les patterns de connecteurs et la gestion des états en profondeur"
      5: "Je définis la stratégie d'intégration de données et je forme les équipes au développement de connecteurs"
      6: "Contributeur Singer/Airbyte, expert ETL reconnu"
    behavioral_indicators:
      1:
        - "Je comprends le pattern Extract-Transform-Load"
        - "Je sais ce qu'est un connecteur de données"
        - "Je peux extraire des données d'un fichier CSV/JSON"
      2:
        - "J'ai développé un connecteur vers une API SaaS"
        - "Je gère la pagination et les erreurs"
        - "Je configure Airbyte ou Fivetran pour une source"
        - "J'extrais des données incrémentales"
      3:
        - "Je crée des connecteurs avec retry et backoff"
        - "Je gère l'état pour les extractions incrémentales"
        - "J'ai développé des connecteurs custom robustes"
        - "Je documente et teste mes connecteurs"
      4:
        - "J'ai créé un framework de connecteurs réutilisable"
        - "Je contribue à Airbyte ou Singer"
        - "Je définis les patterns de connecteurs de l'organisation"
        - "Je forme les équipes au développement de connecteurs"
      5:
        - "Je définis la stratégie d'intégration de données de l'organisation"
        - "J'arbitre les décisions d'architecture de connecteurs pour l'organisation"
        - "Je forme les équipes sur les patterns ETL avancés"
        - "J'établis les standards de qualité et de robustesse des connecteurs"
      6:
        - "Je contribue activement à Airbyte, Singer ou Fivetran"
        - "Je suis reconnu comme expert ETL dans la communauté"
        - "Je publie des connecteurs open-source populaires"
        - "Je donne des conférences sur l'intégration de données"
    resources:
      - url: "https://airbyte.com/data-engineering-resources/data-extraction"
        title: "Data Extraction Guide (Airbyte)"
        type: "tutorial"
      - url: "https://hub.meltano.com/"
        title: "Meltano Hub - Singer Taps & Targets"
        type: "documentation"
      - url: "https://docs.airbyte.com/connector-development/"
        title: "Airbyte Connector Development"
        type: "documentation"
      - url: "https://airbyte.com/tutorials"
        title: "Airbyte Tutorials"
        type: "documentation"
    improvement_tips:
      "1→2": "Développez un connecteur simple vers une API SaaS. Gérez la pagination et les erreurs."
      "2→3": "Ajoutez la gestion d'état et les reprises après échec. Implémentez l'extraction incrémentale."
      "3→4": "Contribuez à Airbyte ou Singer. Créez un framework de connecteurs réutilisable."
